{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Trying the Multilingual BERT models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erki/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create tsv data file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file = open(\"en-sv-train.tsv\", \"w\")\n",
    "\n",
    "with open(\"en-sv.tmx\") as f:\n",
    "    soup = BeautifulSoup(f, 'xml')\n",
    "    sentence_pairs = soup.find_all(\"tu\")\n",
    "    i = 0\n",
    "    for sp in sentence_pairs:\n",
    "        sentences = sp.get_text().split(\"\\n\")\n",
    "        en_sentence = sentences[1]\n",
    "        sv_sentence = sentences[2]\n",
    "        tsv_file.write(en_sentence+\"\\t\"+sv_sentence+\"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Load teacher model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name = \"all-mpnet-base-v2\"\n",
    "teacher_model_name = \"paraphrase-distilroberta-base-v2\"\n",
    "teacher_model = SentenceTransformer(teacher_model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create student model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "student_model_name = \"xlm-roberta-base\"\n",
    "max_seq_length = 256\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Load data from tsv file</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "\n",
    "train_batch_size = 16\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model)\n",
    "train_data.load_data('en-sv-train.tsv')\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['sv'])    # We want to extend the model to these new languages. For language codes, see the header of the train file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Fit the model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/11049 [00:00<?, ?it/s]\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 2.95 GiB total capacity; 1.69 GiB already allocated; 183.69 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(gpus)\n\u001b[1;32m     19\u001b[0m tensorflow\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mset_virtual_device_configuration(gpus[\u001b[39m0\u001b[39m], \n\u001b[1;32m     20\u001b[0m [tensorflow\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mVirtualDeviceConfiguration(memory_limit\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)])\n\u001b[0;32m---> 22\u001b[0m student_model\u001b[39m.\u001b[39;49mfit(train_objectives\u001b[39m=\u001b[39;49m[(train_dataloader, train_loss)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:722\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     loss_value \u001b[39m=\u001b[39m loss_model(features, labels)\n\u001b[0;32m--> 722\u001b[0m     loss_value\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    723\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(loss_model\u001b[39m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m    724\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 2.95 GiB total capacity; 1.69 GiB already allocated; 183.69 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "inference_batch_size = 32           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "assert tensorflow.version.VERSION.startswith('2.')\n",
    "\n",
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "#tensorflow.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "#[tensorflow.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
