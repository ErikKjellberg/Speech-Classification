{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Dataset is supposed to return a speech divided into paragraphs as well as the party.\n",
    "\n",
    "class ParagraphDataset(Dataset):\n",
    "    def __init__(self,speeches,parties,start_index,stop_index,embedding_model=None,file_name=\"\"):\n",
    "        assert len(speeches) == len(parties)\n",
    "        self.length = stop_index-start_index\n",
    "        self.party_order = [\"S\",\"M\",\"SD\",\"V\",\"MP\",\"C\",\"KD\",\"L\"]\n",
    "        self.party_indices = {}\n",
    "        self.embeddings_file_name = file_name\n",
    "        for i, p in enumerate(self.party_order):\n",
    "            self.party_indices[p] = i\n",
    "        print(self.party_indices)\n",
    "        self.party_letters = parties[start_index:stop_index]\n",
    "        self.party_numbers = torch.Tensor([self.party_indices.get(p) for p in self.party_letters]).long()\n",
    "        no_paragraphs = 0\n",
    "        paragraph_list = []\n",
    "        count = 0\n",
    "        for sp in speeches[start_index:stop_index]:\n",
    "            count += 1\n",
    "            for pa in sp:\n",
    "                paragraph_list.append(pa)\n",
    "                no_paragraphs += 1\n",
    "        print(count)\n",
    "        print(\"The amount of paragraphs are\", str(len(paragraph_list)))\n",
    "        if embedding_model!=None:\n",
    "            print(\"Encoding sentences...\")\n",
    "            \n",
    "            paragraph_embeddings = embedding_model.encode(paragraph_list,show_progress_bar=True)\n",
    "            # Save the data to load later\n",
    "            torch.save(paragraph_embeddings,f=self.embeddings_file_name)\n",
    "        else:\n",
    "            print(\"Try to load data from jsonl file...\")\n",
    "            paragraph_embeddings = torch.load(self.embeddings_file_name)\n",
    "        self.speeches = []\n",
    "        self.speech_texts = []\n",
    "        index = 0\n",
    "        for i_s, sp in enumerate(speeches[start_index:stop_index]):\n",
    "            self.speeches.append([])\n",
    "            self.speech_texts.append(sp)\n",
    "            for i_p, pa in enumerate(sp):\n",
    "                self.speeches[-1].append(paragraph_embeddings[index])\n",
    "                index += 1\n",
    "        assert index == no_paragraphs\n",
    "        print(\"Number of paragraphs: \",str(index))\n",
    "        print(\"Number of parties: \",str(len(self.party_numbers)))\n",
    "        print(\"Number of speeches: \",str(len(self.speeches)))\n",
    "\n",
    "\n",
    "    def dump(self,filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            for i in range(self.length):\n",
    "                data = {\"party\":self.party_letters[i],\"party number\":int(self.party_numbers[i]),\"speech\":self.speech_texts[i]}\n",
    "                #data[\"speech embeddings\"] = \n",
    "                json.dump(data, f,indent=2)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    # Get speech at a certain index\n",
    "    def __getitem__(self, index):\n",
    "        # The index list decides which paragraphs to use\n",
    "        speech = self.speeches[index]\n",
    "        #print(speech[0].shape,speech[1].shape)\n",
    "        party = self.party_numbers[index]\n",
    "        return torch.Tensor(speech), torch.Tensor(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "parties = []\n",
    "all_sentences = []\n",
    "all_paragraphs = []\n",
    "\n",
    "possible_parties = [\"S\",\"M\",\"SD\",\"V\",\"MP\",\"C\",\"KD\",\"L\"]\n",
    "\n",
    "# The directory where data is located\n",
    "data_dirs = [\"speech_data/19_20/\",\"speech_data/20_21/\",\"speech_data/21_22/\",\"speech_data/22_23/\"]\n",
    "year_indices = [0,0,0,0]\n",
    "data = []\n",
    "i = 0\n",
    "for year, data_dir in enumerate(data_dirs):\n",
    "    year_indices[year] = len(speeches)\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(data_dir+file,\"r\") as f:\n",
    "                data = json.load(f)\n",
    "                speech = data[\"anforande\"][\"anforandetext\"]\n",
    "                party = data[\"anforande\"][\"parti\"]\n",
    "                # If it is a party who has spoken,\n",
    "                if party in possible_parties:\n",
    "                    soup = BeautifulSoup(speech, 'html.parser')\n",
    "                    paragraphs = []\n",
    "                    for paragraph in soup.find_all(\"p\"):\n",
    "                        text = paragraph.get_text()\n",
    "                        if text != \"\":\n",
    "                            \n",
    "                            \"\"\"sentences = []\n",
    "                            for s in re.split(\"\\. |\\! |\\? \",text):\n",
    "                                if s != \"\":\n",
    "                                    sentences.append(s)\n",
    "                                    all_sentences.append(s)\n",
    "                            if sentences != []:\n",
    "                                all_paragraphs.append(text)\"\"\"\n",
    "                            all_paragraphs.append(text)\n",
    "                            paragraphs.append(text)\n",
    "                    if paragraphs != []:\n",
    "                        parties.append(party)\n",
    "                        speeches.append(paragraphs)\n",
    "\n",
    "print(year_indices)\n",
    "assert len(parties) == len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    #print(\"Before padding (first in embedding):\")\n",
    "    #print([d[0][:,0] for d in data])\n",
    "    input = torch.nn.utils.rnn.pad_sequence([torch.flip(d[0],[0]) for d in data], batch_first=True)\n",
    "    input = torch.flip(input,[1])\n",
    "    #print(\"After padding: \")\n",
    "    #print(input[:,:,0])\n",
    "    output = torch.tensor([d[1] for d in data])\n",
    "    return input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechClassifier(nn.Module):\n",
    "    \"\"\"def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x = torch.flip(x,[1])\n",
    "        #x = x/torch.linalg.vector_norm(x,ord=2,dim=1)\n",
    "        f, (h,c) = self.lstm(x)\n",
    "        h = self.dropout(h)\n",
    "        output = self.fc(h)\n",
    "        return output.squeeze()\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(4*hidden_size, output_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x = nn.functional.normalize(x)\n",
    "        #x = torch.div(x,torch.linalg.vector_norm(x,ord=2,dim=1))\n",
    "        f, (h,c) = self.lstm(x)\n",
    "        #print(\"h: \",str(h.shape))\n",
    "        #print(\"The rows of the hidden state for the first item: \")\n",
    "        #for i in range(4):\n",
    "        #    print(\"Row\",str(i),str(h[i,0,:]))\n",
    "        batch_size = h.shape[1]\n",
    "        h = torch.permute(h,(1,0,2))\n",
    "        h = torch.reshape(h,(batch_size,self.hidden_size*4))\n",
    "        #print(\"Once h is permuted to shape \",str(h.shape),\", it is the following for the first item\")\n",
    "        #print(str(h[0,:]))\n",
    "        #print(str(h.shape))\n",
    "        h = self.dropout(h)\n",
    "        output = self.fc(h)\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(speech_classifier.parameters())\n",
    "epochs = 50\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.00033, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "speech_classifier.train()\n",
    "\n",
    "train_losses = []\n",
    "learning_rates = []\n",
    "val_losses = []\n",
    "#train_losses = torch.load(\"train_losses_model_2.pt\")\n",
    "#val_losses = torch.load(\"val_losses_model_2.pt\")\n",
    "\n",
    "#batch_size = 32\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    speech_classifier.train()\n",
    "    for i, (X_train, y_train) in enumerate(train_loader):\n",
    "        y_pred = speech_classifier(X_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])\n",
    "        train_losses.append(loss.item())\n",
    "    # Evaluate\n",
    "    speech_classifier.eval()\n",
    "    for i, (X_val, y_val) in enumerate(val_loader):\n",
    "        y_pred = speech_classifier(X_val)\n",
    "        loss = loss_fn(y_pred, y_val)\n",
    "        val_losses.append(loss.item())\n",
    "    clear_output()\n",
    "    print(\"Train loss: \"+str(sum(train_losses[-train_stop:])/train_stop))\n",
    "    print(\"Validation loss: \"+str(sum(val_losses[-(val_stop-train_stop):])/(val_stop-train_stop)))\n",
    "    print(\"Learning rate: \"+str(learning_rates[-1]))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
